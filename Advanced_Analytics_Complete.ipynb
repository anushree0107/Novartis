{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NEST 2.0 Clinical Trial - Complete Advanced Analytics Pipeline\n",
                "## Comprehensive Analysis in One Notebook\n",
                "\n",
                "This notebook performs the complete Advanced Analytics pipeline:\n",
                "1. Data Ingestion & Harmonization\n",
                "2. Data Quality Assessment\n",
                "3. Exploratory Data Analysis\n",
                "4. Feature Engineering\n",
                "5. Labeling & Target Definition\n",
                "6. Anomaly Detection\n",
                "7. Predictive Modeling\n",
                "8. Time Series Forecasting\n",
                "9. Explainability\n",
                "10. Monitoring & Drift Detection\n",
                "11. Comprehensive Report\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 1: SETUP & DATA INGESTION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# IMPORTS & CONFIGURATION\n",
                "# ============================================================================\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "from datetime import datetime, timedelta\n",
                "import json\n",
                "import warnings\n",
                "import os\n",
                "\n",
                "# ML imports\n",
                "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
                "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
                "from sklearn.neighbors import LocalOutlierFactor\n",
                "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
                "                             roc_auc_score, classification_report, confusion_matrix,\n",
                "                             precision_recall_curve, roc_curve, average_precision_score,\n",
                "                             mean_absolute_error, mean_squared_error)\n",
                "from scipy import stats\n",
                "\n",
                "# Optional imports\n",
                "try:\n",
                "    from lightgbm import LGBMClassifier\n",
                "    HAS_LGBM = True\n",
                "except: HAS_LGBM = False\n",
                "\n",
                "try:\n",
                "    import shap\n",
                "    HAS_SHAP = True\n",
                "except: HAS_SHAP = False\n",
                "\n",
                "try:\n",
                "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
                "    from statsmodels.tsa.arima.model import ARIMA\n",
                "    HAS_STATSMODELS = True\n",
                "except: HAS_STATSMODELS = False\n",
                "\n",
                "try:\n",
                "    import plotly.express as px\n",
                "    import plotly.graph_objects as go\n",
                "    from plotly.subplots import make_subplots\n",
                "    HAS_PLOTLY = True\n",
                "except: HAS_PLOTLY = False\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = Path(os.getcwd())\n",
                "if BASE_DIR.name == 'notebooks':\n",
                "    BASE_DIR = BASE_DIR.parent\n",
                "\n",
                "RAW_DATA_PATH = BASE_DIR / 'Data' / 'NEST 2.0 data' / 'QC Anonymized Study Files'\n",
                "OLD_PROCESSED_PATH = BASE_DIR / 'processed_data'\n",
                "OUTPUT_PATH = BASE_DIR / 'analytics_output'\n",
                "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "INGESTION_TIME = datetime.utcnow().isoformat() + 'Z'\n",
                "\n",
                "print(f\"Base Directory: {BASE_DIR}\")\n",
                "print(f\"Output Path: {OUTPUT_PATH}\")\n",
                "print(f\"Libraries: LightGBM={HAS_LGBM}, SHAP={HAS_SHAP}, Statsmodels={HAS_STATSMODELS}, Plotly={HAS_PLOTLY}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# DATA LOADING FUNCTIONS\n",
                "# ============================================================================\n",
                "FILE_TYPE_PATTERNS = {\n",
                "    'edrr': ['EDRR', 'Compiled_EDRR'],\n",
                "    'meddra': ['MedDRA', 'GlobalCodingReport_MedDRA'],\n",
                "    'whodd': ['WHODD', 'WHODrug', 'GlobalCodingReport_WHODD'],\n",
                "    'esae': ['eSAE', 'SAE Dashboard', 'SAE_Dashboard'],\n",
                "    'missing_pages': ['Missing_Page', 'Missing Page'],\n",
                "    'visit_projection': ['Visit Projection', 'Visit_Projection']\n",
                "}\n",
                "\n",
                "def load_processed_data(name):\n",
                "    \"\"\"Load data from processed folder.\"\"\"\n",
                "    for path in [OUTPUT_PATH, OLD_PROCESSED_PATH]:\n",
                "        for suffix in ['_processed.csv', '.csv', '_processed.parquet']:\n",
                "            filepath = path / f\"{name}{suffix}\"\n",
                "            if filepath.exists():\n",
                "                if suffix.endswith('.parquet'):\n",
                "                    return pd.read_parquet(filepath)\n",
                "                return pd.read_csv(filepath)\n",
                "    return pd.DataFrame()\n",
                "\n",
                "def standardize_columns(df):\n",
                "    \"\"\"Standardize column names to snake_case.\"\"\"\n",
                "    df.columns = (df.columns.str.strip().str.lower()\n",
                "                  .str.replace(r'[^a-z0-9]+', '_', regex=True).str.strip('_'))\n",
                "    return df\n",
                "\n",
                "def get_study_col(df):\n",
                "    cols = [c for c in df.columns if 'study' in c.lower() and 'source' not in c.lower()]\n",
                "    return cols[0] if cols else '_source_study' if '_source_study' in df.columns else None\n",
                "\n",
                "print(\"Data loading functions defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# LOAD ALL DATASETS\n",
                "# ============================================================================\n",
                "print(\"Loading datasets...\")\n",
                "\n",
                "datasets = {}\n",
                "for name in ['edrr', 'meddra', 'whodd', 'esae_dashboard', 'missing_pages', 'visit_projection']:\n",
                "    df = load_processed_data(name)\n",
                "    if df.empty and name == 'esae_dashboard':\n",
                "        df = load_processed_data('esae')\n",
                "    if not df.empty:\n",
                "        df = standardize_columns(df)\n",
                "    key = 'esae' if 'esae' in name else name\n",
                "    datasets[key] = df\n",
                "    print(f\"  {key}: {df.shape if not df.empty else 'Empty'}\")\n",
                "\n",
                "# Quick access\n",
                "df_edrr = datasets.get('edrr', pd.DataFrame())\n",
                "df_meddra = datasets.get('meddra', pd.DataFrame())\n",
                "df_whodd = datasets.get('whodd', pd.DataFrame())\n",
                "df_esae = datasets.get('esae', pd.DataFrame())\n",
                "df_missing_pages = datasets.get('missing_pages', pd.DataFrame())\n",
                "df_visit = datasets.get('visit_projection', pd.DataFrame())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 2: DATA QUALITY ASSESSMENT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# DATA QUALITY CHECKER\n",
                "# ============================================================================\n",
                "class DataQualityChecker:\n",
                "    def __init__(self, df, name):\n",
                "        self.df = df\n",
                "        self.name = name\n",
                "        self.results = {'dataset': name, 'total_rows': len(df), 'checks': []}\n",
                "    \n",
                "    def check_completeness(self):\n",
                "        for col in self.df.columns:\n",
                "            pct = self.df[col].isnull().mean()\n",
                "            if pct > 0:\n",
                "                sev = 'ERROR' if pct > 0.2 else 'WARN' if pct > 0.05 else 'INFO'\n",
                "                self.results['checks'].append({\n",
                "                    'type': 'completeness', 'column': col, 'value': round(pct*100, 2), 'severity': sev\n",
                "                })\n",
                "        return self\n",
                "    \n",
                "    def get_summary(self):\n",
                "        checks = self.results['checks']\n",
                "        self.results['summary'] = {\n",
                "            'errors': len([c for c in checks if c['severity'] == 'ERROR']),\n",
                "            'warnings': len([c for c in checks if c['severity'] == 'WARN'])\n",
                "        }\n",
                "        return self.results\n",
                "\n",
                "# Run DQ checks\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"DATA QUALITY ASSESSMENT\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "dq_results = {}\n",
                "dq_scores = {}\n",
                "\n",
                "for name, df in datasets.items():\n",
                "    if df.empty:\n",
                "        continue\n",
                "    checker = DataQualityChecker(df, name).check_completeness()\n",
                "    results = checker.get_summary()\n",
                "    dq_results[name] = results\n",
                "    \n",
                "    # Simple DQ score\n",
                "    penalty = results['summary']['errors'] * 10 + results['summary']['warnings'] * 3\n",
                "    score = max(0, 100 - penalty)\n",
                "    dq_scores[name] = score\n",
                "    \n",
                "    status = \"âœ…\" if score >= 80 else \"âš ï¸\" if score >= 60 else \"ğŸ”´\"\n",
                "    print(f\"  {status} {name}: DQ Score = {score}/100 (Errors: {results['summary']['errors']}, Warnings: {results['summary']['warnings']})\")\n",
                "\n",
                "overall_dq = np.mean(list(dq_scores.values())) if dq_scores else 0\n",
                "print(f\"\\n  Overall DQ Score: {overall_dq:.1f}/100\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 3: EXPLORATORY DATA ANALYSIS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# EDA - KEY METRICS SUMMARY\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EXPLORATORY DATA ANALYSIS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# EDRR Analysis\n",
                "if not df_edrr.empty:\n",
                "    issue_col = [c for c in df_edrr.columns if 'issue' in c.lower() and 'count' in c.lower()]\n",
                "    if issue_col:\n",
                "        total_issues = df_edrr[issue_col[0]].sum()\n",
                "        print(f\"\\nğŸ“Š OPEN ISSUES:\")\n",
                "        print(f\"   Total Open Issues: {total_issues:,.0f}\")\n",
                "        print(f\"   Subjects Affected: {len(df_edrr):,}\")\n",
                "\n",
                "# Coding Analysis\n",
                "if not df_meddra.empty:\n",
                "    coding_col = [c for c in df_meddra.columns if 'coding' in c.lower() and 'status' in c.lower()]\n",
                "    if coding_col:\n",
                "        coded = df_meddra[coding_col[0]].str.lower().str.contains('coded term', na=False)\n",
                "        uncoded = df_meddra[coding_col[0]].str.lower().str.contains('uncoded', na=False)\n",
                "        coded_pct = (coded & ~uncoded).mean() * 100\n",
                "        print(f\"\\nğŸ’Š MEDDRA CODING:\")\n",
                "        print(f\"   Total AE Records: {len(df_meddra):,}\")\n",
                "        print(f\"   Coding Rate: {coded_pct:.1f}%\")\n",
                "\n",
                "if not df_whodd.empty:\n",
                "    coding_col = [c for c in df_whodd.columns if 'coding' in c.lower() and 'status' in c.lower()]\n",
                "    if coding_col:\n",
                "        coded = df_whodd[coding_col[0]].str.lower().str.contains('coded term', na=False)\n",
                "        uncoded = df_whodd[coding_col[0]].str.lower().str.contains('uncoded', na=False)\n",
                "        coded_pct = (coded & ~uncoded).mean() * 100\n",
                "        print(f\"\\nğŸ’‰ WHODD CODING:\")\n",
                "        print(f\"   Total Drug Records: {len(df_whodd):,}\")\n",
                "        print(f\"   Coding Rate: {coded_pct:.1f}%\")\n",
                "\n",
                "# Safety Events\n",
                "if not df_esae.empty:\n",
                "    print(f\"\\nâš ï¸ SAFETY EVENTS:\")\n",
                "    print(f\"   Total Records: {len(df_esae):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# EDA VISUALIZATIONS\n",
                "# ============================================================================\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# 1. DQ Scores\n",
                "ax1 = axes[0, 0]\n",
                "if dq_scores:\n",
                "    colors = ['#2ecc71' if s >= 80 else '#f39c12' if s >= 60 else '#e74c3c' for s in dq_scores.values()]\n",
                "    ax1.barh(list(dq_scores.keys()), list(dq_scores.values()), color=colors)\n",
                "    ax1.set_xlabel('DQ Score')\n",
                "    ax1.set_title('Data Quality Scores by Dataset')\n",
                "    ax1.set_xlim(0, 100)\n",
                "\n",
                "# 2. Record counts\n",
                "ax2 = axes[0, 1]\n",
                "record_counts = {k: len(v) for k, v in datasets.items() if not v.empty}\n",
                "if record_counts:\n",
                "    ax2.bar(record_counts.keys(), record_counts.values(), color='steelblue')\n",
                "    ax2.set_ylabel('Record Count')\n",
                "    ax2.set_title('Records by Dataset')\n",
                "    ax2.tick_params(axis='x', rotation=45)\n",
                "\n",
                "# 3. Issues by study (if available)\n",
                "ax3 = axes[1, 0]\n",
                "if not df_edrr.empty:\n",
                "    study_col = get_study_col(df_edrr)\n",
                "    issue_col = [c for c in df_edrr.columns if 'issue' in c.lower() and 'count' in c.lower()]\n",
                "    if study_col and issue_col:\n",
                "        study_issues = df_edrr.groupby(study_col)[issue_col[0]].sum().sort_values(ascending=False).head(10)\n",
                "        ax3.barh(study_issues.index, study_issues.values, color='indianred')\n",
                "        ax3.set_xlabel('Total Issues')\n",
                "        ax3.set_title('Top 10 Studies by Open Issues')\n",
                "        ax3.invert_yaxis()\n",
                "\n",
                "# 4. Safety events by study\n",
                "ax4 = axes[1, 1]\n",
                "if not df_esae.empty:\n",
                "    study_col = get_study_col(df_esae)\n",
                "    if study_col:\n",
                "        study_safety = df_esae.groupby(study_col).size().sort_values(ascending=False).head(10)\n",
                "        ax4.barh(study_safety.index, study_safety.values, color='coral')\n",
                "        ax4.set_xlabel('Safety Events')\n",
                "        ax4.set_title('Top 10 Studies by Safety Events')\n",
                "        ax4.invert_yaxis()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_PATH / 'eda_overview.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"\\nSaved: {OUTPUT_PATH / 'eda_overview.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 4: FEATURE ENGINEERING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# BUILD STUDY-LEVEL FEATURES\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"FEATURE ENGINEERING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Get all studies\n",
                "all_studies = set()\n",
                "for df in datasets.values():\n",
                "    if not df.empty:\n",
                "        col = get_study_col(df)\n",
                "        if col:\n",
                "            all_studies.update(df[col].dropna().unique())\n",
                "\n",
                "print(f\"Found {len(all_studies)} unique studies\")\n",
                "\n",
                "# Build features\n",
                "study_features = []\n",
                "\n",
                "for study in all_studies:\n",
                "    feat = {'study': study}\n",
                "    \n",
                "    # EDRR features\n",
                "    if not df_edrr.empty:\n",
                "        col = get_study_col(df_edrr)\n",
                "        issue_col = [c for c in df_edrr.columns if 'issue' in c.lower() and 'count' in c.lower()]\n",
                "        if col and issue_col:\n",
                "            study_data = df_edrr[df_edrr[col] == study]\n",
                "            feat['total_open_issues'] = study_data[issue_col[0]].sum() if len(study_data) > 0 else 0\n",
                "            feat['subjects_with_issues'] = len(study_data)\n",
                "    \n",
                "    # MedDRA features\n",
                "    if not df_meddra.empty:\n",
                "        col = get_study_col(df_meddra)\n",
                "        if col:\n",
                "            study_data = df_meddra[df_meddra[col] == study]\n",
                "            feat['total_ae_records'] = len(study_data)\n",
                "            coding_col = [c for c in df_meddra.columns if 'coding' in c.lower() and 'status' in c.lower()]\n",
                "            if coding_col and len(study_data) > 0:\n",
                "                coded = (study_data[coding_col[0]].str.lower().str.contains('coded term', na=False) & \n",
                "                        ~study_data[coding_col[0]].str.lower().str.contains('uncoded', na=False)).sum()\n",
                "                feat['ae_coding_rate'] = coded / len(study_data) if len(study_data) > 0 else 1\n",
                "    \n",
                "    # WHODD features\n",
                "    if not df_whodd.empty:\n",
                "        col = get_study_col(df_whodd)\n",
                "        if col:\n",
                "            study_data = df_whodd[df_whodd[col] == study]\n",
                "            feat['total_drug_records'] = len(study_data)\n",
                "    \n",
                "    # Safety features\n",
                "    if not df_esae.empty:\n",
                "        col = get_study_col(df_esae)\n",
                "        if col:\n",
                "            study_data = df_esae[df_esae[col] == study]\n",
                "            feat['total_safety_events'] = len(study_data)\n",
                "    \n",
                "    # Missing pages\n",
                "    if not df_missing_pages.empty:\n",
                "        col = get_study_col(df_missing_pages)\n",
                "        if col:\n",
                "            study_data = df_missing_pages[df_missing_pages[col] == study]\n",
                "            feat['missing_pages_count'] = len(study_data)\n",
                "    \n",
                "    study_features.append(feat)\n",
                "\n",
                "df_features = pd.DataFrame(study_features).fillna(0)\n",
                "print(f\"\\nFeature matrix shape: {df_features.shape}\")\n",
                "print(f\"Features: {df_features.columns.tolist()}\")\n",
                "df_features.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 5: LABELING & TARGET DEFINITION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CREATE LABELS\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"LABELING & TARGET DEFINITION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "df_labeled = df_features.copy()\n",
                "\n",
                "# High risk study label\n",
                "conditions = []\n",
                "if 'total_open_issues' in df_labeled.columns:\n",
                "    conditions.append(df_labeled['total_open_issues'] >= 50)\n",
                "if 'total_safety_events' in df_labeled.columns:\n",
                "    conditions.append(df_labeled['total_safety_events'] >= 100)\n",
                "if 'missing_pages_count' in df_labeled.columns:\n",
                "    conditions.append(df_labeled['missing_pages_count'] >= 100)\n",
                "\n",
                "if conditions:\n",
                "    df_labeled['label_high_risk'] = np.where(np.any(conditions, axis=0), 1, 0)\n",
                "else:\n",
                "    df_labeled['label_high_risk'] = 0\n",
                "\n",
                "# Coding incomplete label\n",
                "if 'ae_coding_rate' in df_labeled.columns:\n",
                "    df_labeled['label_coding_incomplete'] = (df_labeled['ae_coding_rate'] < 0.98).astype(int)\n",
                "else:\n",
                "    df_labeled['label_coding_incomplete'] = 0\n",
                "\n",
                "print(f\"\\nLabels created:\")\n",
                "print(f\"  label_high_risk: {df_labeled['label_high_risk'].sum()} positive ({df_labeled['label_high_risk'].mean()*100:.1f}%)\")\n",
                "print(f\"  label_coding_incomplete: {df_labeled['label_coding_incomplete'].sum()} positive ({df_labeled['label_coding_incomplete'].mean()*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 6: ANOMALY DETECTION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ANOMALY DETECTION\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ANOMALY DETECTION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Prepare numeric features\n",
                "feature_cols = [c for c in df_labeled.columns if c not in ['study', 'label_high_risk', 'label_coding_incomplete']\n",
                "                and df_labeled[c].dtype in ['int64', 'float64']]\n",
                "\n",
                "if len(feature_cols) > 0 and len(df_labeled) > 5:\n",
                "    X = df_labeled[feature_cols].fillna(0)\n",
                "    scaler = StandardScaler()\n",
                "    X_scaled = scaler.fit_transform(X)\n",
                "    \n",
                "    # Z-Score\n",
                "    z_scores = np.abs(stats.zscore(X, nan_policy='omit'))\n",
                "    z_outliers = (z_scores > 3).any(axis=1)\n",
                "    df_labeled['anomaly_zscore'] = z_outliers.astype(int)\n",
                "    \n",
                "    # Isolation Forest\n",
                "    iso = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
                "    iso_pred = iso.fit_predict(X_scaled)\n",
                "    df_labeled['anomaly_iforest'] = (iso_pred == -1).astype(int)\n",
                "    \n",
                "    # LOF\n",
                "    n_neighbors = min(20, max(5, len(df_labeled) // 3))\n",
                "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.1)\n",
                "    lof_pred = lof.fit_predict(X_scaled)\n",
                "    df_labeled['anomaly_lof'] = (lof_pred == -1).astype(int)\n",
                "    \n",
                "    # Ensemble\n",
                "    anomaly_cols = ['anomaly_zscore', 'anomaly_iforest', 'anomaly_lof']\n",
                "    df_labeled['anomaly_count'] = df_labeled[anomaly_cols].sum(axis=1)\n",
                "    df_labeled['anomaly_ensemble'] = (df_labeled['anomaly_count'] >= 2).astype(int)\n",
                "    \n",
                "    print(f\"\\nğŸ” Anomalies Detected:\")\n",
                "    print(f\"   Z-Score: {df_labeled['anomaly_zscore'].sum()}\")\n",
                "    print(f\"   Isolation Forest: {df_labeled['anomaly_iforest'].sum()}\")\n",
                "    print(f\"   LOF: {df_labeled['anomaly_lof'].sum()}\")\n",
                "    print(f\"   Ensemble (>=2 methods): {df_labeled['anomaly_ensemble'].sum()}\")\n",
                "    \n",
                "    # Top anomalies\n",
                "    if df_labeled['anomaly_ensemble'].sum() > 0:\n",
                "        print(f\"\\nâš ï¸ Anomalous Studies:\")\n",
                "        for _, row in df_labeled[df_labeled['anomaly_ensemble'] == 1].head(5).iterrows():\n",
                "            print(f\"   ğŸ”´ {row['study']}: detected by {row['anomaly_count']} methods\")\n",
                "else:\n",
                "    print(\"Insufficient data for anomaly detection.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 7: PREDICTIVE MODELING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# PREDICTIVE MODELING\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"PREDICTIVE MODELING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Prepare features and target\n",
                "exclude_cols = ['study', 'label_high_risk', 'label_coding_incomplete', 'anomaly_zscore', \n",
                "                'anomaly_iforest', 'anomaly_lof', 'anomaly_count', 'anomaly_ensemble']\n",
                "feature_cols = [c for c in df_labeled.columns if c not in exclude_cols \n",
                "                and df_labeled[c].dtype in ['int64', 'float64']]\n",
                "\n",
                "TARGET = 'label_high_risk'\n",
                "model_results = []\n",
                "\n",
                "if TARGET in df_labeled.columns and len(feature_cols) > 0 and len(df_labeled) > 10:\n",
                "    X = df_labeled[feature_cols].fillna(0)\n",
                "    y = df_labeled[TARGET]\n",
                "    \n",
                "    # Scale\n",
                "    scaler = StandardScaler()\n",
                "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_cols)\n",
                "    \n",
                "    # Split\n",
                "    try:\n",
                "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42, stratify=y)\n",
                "    except:\n",
                "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
                "    \n",
                "    print(f\"\\nTraining: {len(X_train)} samples, Test: {len(X_test)} samples\")\n",
                "    print(f\"Target: {TARGET}\")\n",
                "    \n",
                "    # Define models\n",
                "    models = {\n",
                "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
                "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced'),\n",
                "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
                "    }\n",
                "    if HAS_LGBM:\n",
                "        models['LightGBM'] = LGBMClassifier(n_estimators=100, max_depth=5, random_state=42, class_weight='balanced', verbose=-1)\n",
                "    \n",
                "    # Train and evaluate\n",
                "    trained_models = {}\n",
                "    print(f\"\\nğŸ“Š Model Performance:\")\n",
                "    \n",
                "    for name, model in models.items():\n",
                "        try:\n",
                "            model.fit(X_train, y_train)\n",
                "            trained_models[name] = model\n",
                "            \n",
                "            y_pred = model.predict(X_test)\n",
                "            y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
                "            \n",
                "            auc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0.5\n",
                "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
                "            \n",
                "            model_results.append({'Model': name, 'AUC-ROC': auc, 'F1': f1})\n",
                "            print(f\"   {name}: AUC={auc:.3f}, F1={f1:.3f}\")\n",
                "        except Exception as e:\n",
                "            print(f\"   {name}: Error - {str(e)[:30]}\")\n",
                "    \n",
                "    # Best model\n",
                "    if model_results:\n",
                "        best = max(model_results, key=lambda x: x['AUC-ROC'])\n",
                "        print(f\"\\nğŸ† Best Model: {best['Model']} (AUC={best['AUC-ROC']:.3f})\")\n",
                "        \n",
                "        # Add predictions to dataframe\n",
                "        best_model = trained_models[best['Model']]\n",
                "        df_labeled['prediction'] = best_model.predict(X_scaled)\n",
                "        if hasattr(best_model, 'predict_proba'):\n",
                "            df_labeled['probability'] = best_model.predict_proba(X_scaled)[:, 1]\n",
                "else:\n",
                "    print(\"Insufficient data for predictive modeling.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 8: TIME SERIES FORECASTING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# TIME SERIES FORECASTING (Synthetic Demo)\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TIME SERIES FORECASTING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Create synthetic time series\n",
                "np.random.seed(42)\n",
                "n_days = 90\n",
                "dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
                "\n",
                "base_value = df_labeled['total_open_issues'].sum() / n_days if 'total_open_issues' in df_labeled.columns else 10\n",
                "trend = np.linspace(0, 0.1 * base_value, n_days)\n",
                "seasonality = 0.2 * base_value * np.sin(2 * np.pi * np.arange(n_days) / 7)\n",
                "noise = 0.1 * base_value * np.random.randn(n_days)\n",
                "ts_data = np.maximum(base_value + trend + seasonality + noise, 0)\n",
                "\n",
                "df_ts = pd.DataFrame({'date': dates, 'value': ts_data}).set_index('date')\n",
                "\n",
                "# Split\n",
                "train_size = int(len(df_ts) * 0.8)\n",
                "train_ts = df_ts.iloc[:train_size]\n",
                "test_ts = df_ts.iloc[train_size:]\n",
                "\n",
                "print(f\"\\nTime series: {len(df_ts)} days\")\n",
                "print(f\"Train: {len(train_ts)}, Test: {len(test_ts)}\")\n",
                "\n",
                "# Simple forecasts\n",
                "forecasts = {}\n",
                "forecasts['Naive'] = np.full(len(test_ts), train_ts['value'].iloc[-1])\n",
                "forecasts['Mean'] = np.full(len(test_ts), train_ts['value'].mean())\n",
                "\n",
                "# Linear trend\n",
                "X_tr = np.arange(len(train_ts)).reshape(-1, 1)\n",
                "lr = LinearRegression().fit(X_tr, train_ts['value'])\n",
                "X_te = np.arange(len(train_ts), len(train_ts) + len(test_ts)).reshape(-1, 1)\n",
                "forecasts['Linear'] = lr.predict(X_te)\n",
                "\n",
                "# ARIMA if available\n",
                "if HAS_STATSMODELS:\n",
                "    try:\n",
                "        arima = ARIMA(train_ts['value'], order=(2, 1, 2)).fit()\n",
                "        forecasts['ARIMA'] = arima.forecast(len(test_ts)).values\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "# Evaluate\n",
                "print(f\"\\nğŸ“ˆ Forecast Evaluation:\")\n",
                "for name, pred in forecasts.items():\n",
                "    mae = mean_absolute_error(test_ts['value'], pred)\n",
                "    print(f\"   {name}: MAE = {mae:.2f}\")\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.plot(train_ts.index, train_ts['value'], 'b-', label='Train', alpha=0.7)\n",
                "plt.plot(test_ts.index, test_ts['value'], 'k-', label='Actual', linewidth=2)\n",
                "for name, pred in forecasts.items():\n",
                "    plt.plot(test_ts.index, pred, '--', label=name, alpha=0.7)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Value')\n",
                "plt.title('Time Series Forecast Comparison')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_PATH / 'forecast.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 9: EXPLAINABILITY"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# MODEL EXPLAINABILITY\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"MODEL EXPLAINABILITY\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "if 'trained_models' in dir() and trained_models:\n",
                "    # Get best tree-based model for feature importance\n",
                "    best_tree = None\n",
                "    for name in ['LightGBM', 'Random Forest', 'Gradient Boosting']:\n",
                "        if name in trained_models:\n",
                "            best_tree = trained_models[name]\n",
                "            best_name = name\n",
                "            break\n",
                "    \n",
                "    if best_tree and hasattr(best_tree, 'feature_importances_'):\n",
                "        importance = pd.DataFrame({\n",
                "            'feature': feature_cols,\n",
                "            'importance': best_tree.feature_importances_\n",
                "        }).sort_values('importance', ascending=False)\n",
                "        \n",
                "        print(f\"\\nğŸ” Top 10 Features ({best_name}):\")\n",
                "        for _, row in importance.head(10).iterrows():\n",
                "            print(f\"   - {row['feature']}: {row['importance']:.4f}\")\n",
                "        \n",
                "        # Plot\n",
                "        plt.figure(figsize=(10, 6))\n",
                "        top_15 = importance.head(15)\n",
                "        plt.barh(range(len(top_15)), top_15['importance'], color='steelblue')\n",
                "        plt.yticks(range(len(top_15)), top_15['feature'])\n",
                "        plt.xlabel('Importance')\n",
                "        plt.title(f'Feature Importance ({best_name})')\n",
                "        plt.gca().invert_yaxis()\n",
                "        plt.tight_layout()\n",
                "        plt.savefig(OUTPUT_PATH / 'feature_importance.png', dpi=150, bbox_inches='tight')\n",
                "        plt.show()\n",
                "    \n",
                "    # High-risk explanations\n",
                "    if 'probability' in df_labeled.columns:\n",
                "        print(f\"\\nâš ï¸ Top 5 High-Risk Studies:\")\n",
                "        for _, row in df_labeled.nlargest(5, 'probability').iterrows():\n",
                "            prob = row['probability'] * 100\n",
                "            print(f\"   ğŸ”´ {row['study']}: {prob:.1f}% risk\")\n",
                "else:\n",
                "    print(\"No trained models available for explainability.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 10: MONITORING & DRIFT DETECTION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# MONITORING & DRIFT DETECTION\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"MONITORING & DRIFT DETECTION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def calculate_psi(ref, prod, n_bins=10):\n",
                "    \"\"\"Calculate Population Stability Index.\"\"\"\n",
                "    ref, prod = ref.dropna(), prod.dropna()\n",
                "    if len(ref) == 0 or len(prod) == 0:\n",
                "        return 0.0\n",
                "    bins = np.linspace(min(ref.min(), prod.min()), max(ref.max(), prod.max()), n_bins + 1)\n",
                "    ref_pct = (np.histogram(ref, bins)[0] + 0.001) / (len(ref) + 0.001 * n_bins)\n",
                "    prod_pct = (np.histogram(prod, bins)[0] + 0.001) / (len(prod) + 0.001 * n_bins)\n",
                "    return np.sum((prod_pct - ref_pct) * np.log(prod_pct / ref_pct))\n",
                "\n",
                "# Simulate reference vs production split\n",
                "if len(df_labeled) > 10:\n",
                "    split = int(len(df_labeled) * 0.7)\n",
                "    ref_df = df_labeled.iloc[:split]\n",
                "    prod_df = df_labeled.iloc[split:]\n",
                "    \n",
                "    drift_cols = [c for c in feature_cols if c in df_labeled.columns][:10]\n",
                "    drift_results = []\n",
                "    \n",
                "    for col in drift_cols:\n",
                "        psi = calculate_psi(ref_df[col], prod_df[col])\n",
                "        status = 'HIGH DRIFT' if psi > 0.2 else 'MODERATE' if psi > 0.1 else 'OK'\n",
                "        drift_results.append({'feature': col, 'psi': psi, 'status': status})\n",
                "    \n",
                "    drift_df = pd.DataFrame(drift_results).sort_values('psi', ascending=False)\n",
                "    \n",
                "    print(f\"\\nğŸ“Š Drift Analysis (Reference: {len(ref_df)}, Production: {len(prod_df)}):\")\n",
                "    high_drift = (drift_df['status'] == 'HIGH DRIFT').sum()\n",
                "    moderate_drift = (drift_df['status'] == 'MODERATE').sum()\n",
                "    print(f\"   High Drift: {high_drift} features\")\n",
                "    print(f\"   Moderate Drift: {moderate_drift} features\")\n",
                "    print(f\"   OK: {(drift_df['status'] == 'OK').sum()} features\")\n",
                "    \n",
                "    # Retrain decision\n",
                "    max_psi = drift_df['psi'].max()\n",
                "    if max_psi > 0.2 or high_drift > 2:\n",
                "        print(f\"\\nâš ï¸ RETRAIN RECOMMENDED (Max PSI: {max_psi:.3f})\")\n",
                "    else:\n",
                "        print(f\"\\nâœ… No retrain needed (Max PSI: {max_psi:.3f})\")\n",
                "else:\n",
                "    print(\"Insufficient data for drift detection.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PART 11: COMPREHENSIVE REPORT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# COMPREHENSIVE ANALYTICS REPORT\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"COMPREHENSIVE ANALYTICS REPORT\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "report = {\n",
                "    'generated_at': datetime.utcnow().isoformat() + 'Z',\n",
                "    'summary': {\n",
                "        'total_studies': len(all_studies),\n",
                "        'overall_dq_score': round(overall_dq, 1),\n",
                "        'anomalies_detected': int(df_labeled['anomaly_ensemble'].sum()) if 'anomaly_ensemble' in df_labeled.columns else 0,\n",
                "        'best_model': max(model_results, key=lambda x: x['AUC-ROC'])['Model'] if model_results else 'N/A'\n",
                "    },\n",
                "    'dq_scores': dq_scores,\n",
                "    'model_results': model_results\n",
                "}\n",
                "\n",
                "print(f\"\"\"\n",
                "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
                "â•‘                    EXECUTIVE SUMMARY                              â•‘\n",
                "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
                "â•‘  ğŸ“Š Studies Analyzed:         {report['summary']['total_studies']:<35} â•‘\n",
                "â•‘  ğŸ“ˆ Overall DQ Score:         {report['summary']['overall_dq_score']:<35} â•‘\n",
                "â•‘  âš ï¸ Anomalies Detected:       {report['summary']['anomalies_detected']:<35} â•‘\n",
                "â•‘  ğŸ¤– Best Model:               {report['summary']['best_model']:<35} â•‘\n",
                "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\"\"\")\n",
                "\n",
                "# Save report\n",
                "with open(OUTPUT_PATH / 'analytics_report.json', 'w') as f:\n",
                "    json.dump(report, f, indent=2, default=str)\n",
                "\n",
                "# Save labeled data\n",
                "df_labeled.to_csv(OUTPUT_PATH / 'study_analytics.csv', index=False)\n",
                "\n",
                "print(f\"\\nğŸ“ Files saved to: {OUTPUT_PATH}\")\n",
                "for f in OUTPUT_PATH.glob('*'):\n",
                "    print(f\"   - {f.name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# RECOMMENDATIONS\n",
                "# ============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"RECOMMENDATIONS & ACTION ITEMS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "print(\"\\nğŸ”´ HIGH PRIORITY:\")\n",
                "if 'anomaly_ensemble' in df_labeled.columns and df_labeled['anomaly_ensemble'].sum() > 0:\n",
                "    print(f\"   â€¢ Investigate {df_labeled['anomaly_ensemble'].sum()} anomalous studies\")\n",
                "if overall_dq < 80:\n",
                "    print(f\"   â€¢ Address data quality issues (current score: {overall_dq:.1f}/100)\")\n",
                "\n",
                "print(\"\\nğŸŸ¡ MEDIUM PRIORITY:\")\n",
                "print(\"   â€¢ Implement automated DQ monitoring\")\n",
                "print(\"   â€¢ Set up model retraining pipeline\")\n",
                "print(\"   â€¢ Create stakeholder dashboards\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… ADVANCED ANALYTICS PIPELINE COMPLETE!\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}